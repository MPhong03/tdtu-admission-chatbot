{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e5275e",
   "metadata": {},
   "source": [
    "# HUẤN LUYỆN MÔ HÌNH NER V2 - HUGGINGFACE TRANSFORMERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce2338",
   "metadata": {},
   "source": [
    "## I. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf520d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "W0510 23:12:50.518000 15392 torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ae0af0",
   "metadata": {},
   "source": [
    "## II. Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ca078",
   "metadata": {},
   "source": [
    "### 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a0619c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/ner_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)[\"dataset\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c3f74",
   "metadata": {},
   "source": [
    "### 2. Flatten and convert to token classification format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85787133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, tokenizer):\n",
    "    tokenized_inputs = []\n",
    "    label_all_tokens = True\n",
    "    label2id = {\"O\": 0, \"B-Major\": 1, \"I-Major\": 2, \"B-Programme\": 3, \"I-Programme\": 4, \"B-Group\": 5, \"I-Group\": 6}\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "    texts = [item[\"text\"] for item in data]\n",
    "    annotations = [item[\"entities\"] for item in data]\n",
    "\n",
    "    for text, entities in zip(texts, annotations):\n",
    "        tokens = tokenizer(text, truncation=True, is_split_into_words=False)\n",
    "        word_ids = tokens.word_ids()\n",
    "\n",
    "        labels = [\"O\"] * len(tokens.input_ids)\n",
    "        for start, end, label in entities:\n",
    "            for idx, word_id in enumerate(word_ids):\n",
    "                if word_id is None:\n",
    "                    continue\n",
    "                token_start = tokens.token_to_chars(idx).start\n",
    "                token_end = tokens.token_to_chars(idx).end\n",
    "                if token_start >= start and token_end <= end:\n",
    "                    prefix = \"B-\" if token_start == start else \"I-\"\n",
    "                    labels[idx] = prefix + label\n",
    "\n",
    "        tokens[\"labels\"] = [label2id.get(l, 0) for l in labels]\n",
    "        tokenized_inputs.append(tokens)\n",
    "\n",
    "    return Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"] for x in tokenized_inputs],\n",
    "        \"attention_mask\": [x[\"attention_mask\"] for x in tokenized_inputs],\n",
    "        \"labels\": [x[\"labels\"] for x in tokenized_inputs],\n",
    "    }), label2id, id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd98adb",
   "metadata": {},
   "source": [
    "### 3. Initialize tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb2fff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab724c",
   "metadata": {},
   "source": [
    "### 4. Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb4ae533",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, label2id, id2label = preprocess_data(raw_data, tokenizer)\n",
    "dataset = dataset.train_test_split(test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a63f5",
   "metadata": {},
   "source": [
    "### 5. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1fc7b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797ba19",
   "metadata": {},
   "source": [
    "### 6. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b8038b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d762a09",
   "metadata": {},
   "source": [
    "## III. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bcfde42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phonyy\\AppData\\Local\\Temp\\ipykernel_20552\\359412530.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    \"ner-bert-multilingual\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27b5cb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17814' max='17814' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17814/17814 1:26:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.999823</td>\n",
       "      <td>0.999823</td>\n",
       "      <td>0.999823</td>\n",
       "      <td>0.999989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17814, training_loss=0.001700691110901497, metrics={'train_runtime': 5174.713, 'train_samples_per_second': 55.076, 'train_steps_per_second': 3.443, 'total_flos': 5062257405987936.0, 'train_loss': 0.001700691110901497, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9e4d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "trainer.save_model(\"ner-bert-multilingual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb29fe96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.822525731171481e-06, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 15.994, 'eval_samples_per_second': 312.618, 'eval_steps_per_second': 19.57, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af857f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Programme', 'tokens': ['Liên', 'kết', 'quốc', 'tế']}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"ner-bert-multilingual\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ner-bert-multilingual\")\n",
    "id2label = model.config.id2label\n",
    "\n",
    "def predict_entities(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    entities = []\n",
    "\n",
    "    current = None\n",
    "    for token, pred_id in zip(tokens, predictions):\n",
    "        label = id2label[pred_id]\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current:\n",
    "                entities.append(current)\n",
    "            current = {\"label\": label[2:], \"tokens\": [token]}\n",
    "        elif label.startswith(\"I-\") and current:\n",
    "            current[\"tokens\"].append(token)\n",
    "        else:\n",
    "            if current:\n",
    "                entities.append(current)\n",
    "                current = None\n",
    "    if current:\n",
    "        entities.append(current)\n",
    "\n",
    "    return entities\n",
    "\n",
    "# Ví dụ test\n",
    "text = \"Liên kết quốc tế\"\n",
    "print(predict_entities(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f5345f",
   "metadata": {},
   "source": [
    "## IV. Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d2b86ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"pip install optimum[onnxruntime] -q\")\n",
    "from optimum.onnxruntime import ORTModelForTokenClassification\n",
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a266fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W0511 06:57:35.238000 7064 torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "from optimum.exporters.onnx import main_export\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e61dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_export(\n",
    "    model_name_or_path=\"ner-bert-multilingual\",    # Đường dẫn mô hình đã fine-tune\n",
    "    task=\"token-classification\",\n",
    "    output=Path(\"onnx/ner_model_opset14\"),         # Thư mục xuất\n",
    "    opset=16,                                      # ✅ bắt buộc dùng >= 14\n",
    "    device=\"cpu\"                                   # hoặc \"cuda\" nếu dùng GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53e00f",
   "metadata": {},
   "source": [
    "Mô hình đang overfitting hoặc đánh giá sai lệch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
