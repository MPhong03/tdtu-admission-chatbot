{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8d1a24",
   "metadata": {},
   "source": [
    "# GIAI ĐOẠN 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd5f6c",
   "metadata": {},
   "source": [
    "# DỮ LIỆU NGÀNH HỌC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6361778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã xuất ra file tdtu_majors.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "\n",
    "# URL của trang danh mục ngành học\n",
    "url = 'https://admission.tdtu.edu.vn/dai-hoc/nganh-hoc'\n",
    "\n",
    "# Gửi yêu cầu GET đến trang web\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Danh sách lưu kết quả\n",
    "result = []\n",
    "\n",
    "# Tìm tất cả các nhóm ngành\n",
    "groups = soup.find_all('div', class_='ts-ng-und')\n",
    "\n",
    "for group in groups:\n",
    "    # Lấy tên nhóm ngành\n",
    "    group_name_tag = group.find(class_='ts-ng-und-td')\n",
    "    if not group_name_tag:\n",
    "        continue\n",
    "\n",
    "    group_name = group_name_tag.get_text(strip=True)\n",
    "\n",
    "    # Tìm tất cả các ngành trong nhóm\n",
    "    majors = group.find_all('a')\n",
    "    major_list = []\n",
    "    for major in majors:\n",
    "        major_name = major.get_text(strip=True)\n",
    "        major_link = major['href']\n",
    "        major_list.append({\n",
    "            'name': major_name,\n",
    "            'link': major_link\n",
    "        })\n",
    "\n",
    "    result.append({\n",
    "        'group_name': group_name,\n",
    "        'majors': major_list\n",
    "    })\n",
    "\n",
    "# Ghi ra file JSON\n",
    "os.makedirs('phase1', exist_ok=True)\n",
    "with open('phase1/tdtu_majors.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Đã xuất ra file tdtu_majors.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c42f674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hoàn tất! Đã lưu vào thư mục 'output'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "BASE_URL = 'https://admission.tdtu.edu.vn/dai-hoc/nganh-hoc'\n",
    "OUTPUT_DIR = 'phase1'\n",
    "DETAILS_DIR = 'phase1/details'\n",
    "\n",
    "def get_text_safe(element):\n",
    "    return element.get_text(strip=True) if element else ''\n",
    "\n",
    "def crawl_major_detail(url):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    data['name'] = get_text_safe(soup.find('h2', class_='title'))\n",
    "\n",
    "    description_block = soup.find('div', class_='block-nganh-1-content')\n",
    "    data['description'] = '\\n'.join(get_text_safe(p) for p in description_block.find_all('p')) if description_block else ''\n",
    "\n",
    "    reasons = []\n",
    "    for block in soup.select('div.gsc-column'):\n",
    "        title = block.select_one('div.highlight_content > div.title')\n",
    "        if title:\n",
    "            reasons.append(get_text_safe(title))\n",
    "    data['reasons'] = reasons\n",
    "\n",
    "    data['programs'] = []\n",
    "\n",
    "    # Tìm block chứa danh sách tab\n",
    "    tabs_container = soup.select_one('div.gsc-tabs, div.nganh-link-admission')\n",
    "    if tabs_container:\n",
    "        tab_list = tabs_container.find('ul')\n",
    "        if tab_list:\n",
    "            for li in tab_list.find_all('li'):\n",
    "                a = li.find('a')\n",
    "                if a:\n",
    "                    data['programs'].append({\n",
    "                        'tab': get_text_safe(a),\n",
    "                        'major_code': '',\n",
    "                        'description': '',\n",
    "                        'content': {}\n",
    "                    })\n",
    "\n",
    "    tab_contents = []\n",
    "    if soup.select_one('div.gsc-tabs'):\n",
    "        tab_contents = soup.select('div.gsc-tabs div.tab-content > .tab-pane')\n",
    "    elif soup.select_one('div.nganh-link-admission'):\n",
    "        # Tìm tất cả các <div> con trong .nganh-link-admission mà KHÔNG chứa <ul> (chỉ lấy content tab)\n",
    "        nganh_container = soup.select_one('div.nganh-link-admission')\n",
    "        tab_contents = [\n",
    "            div for div in nganh_container.find_all('div', recursive=False)\n",
    "            if not div.find('ul') and div.get('id')  # thường là #tieu-chuan, #tien-tien, ...\n",
    "        ]\n",
    "\n",
    "    for i, content in enumerate(tab_contents):\n",
    "        if i >= len(data['programs']):\n",
    "            continue\n",
    "        program = data['programs'][i]\n",
    "        tt_block = content.find('div', class_='tab-chuong-trinh-tt')\n",
    "        if tt_block:\n",
    "            strongs = [get_text_safe(s) for s in tt_block.find_all('strong') if get_text_safe(s)]\n",
    "            full_text = ' '.join(strongs).strip()\n",
    "\n",
    "            # Regex tìm tên ngành và mã ngành, ví dụ:\n",
    "            # - Kỹ thuật phần mềm - Mã ngành: F7480103\n",
    "            # - Luật (Chuyên ngành...) - Mã ngành: F 7380101\n",
    "            match = re.search(r'^(.*?)\\s*[-–—]?\\s*Mã ngành[:：]?\\s*([A-Z]{0,2}\\s*\\d{5,})', full_text, re.IGNORECASE)\n",
    "\n",
    "            if match:\n",
    "                program['name'] = match.group(1).strip()\n",
    "                program['major_code'] = match.group(2).replace(\" \", \"\").strip()  # Loại bỏ khoảng trắng\n",
    "            else:\n",
    "                program['name'] = full_text\n",
    "                program['major_code'] = ''\n",
    "\n",
    "        program['description'] = '\\n'.join(get_text_safe(p) for p in content.find_all('p') if get_text_safe(p))\n",
    "        ct_block = content.find('div', class_='tab-chuong-trinh')\n",
    "        if ct_block:\n",
    "            for section in ct_block.find_all('div', class_='tab-ct'):\n",
    "                for flex in section.find_all('div', class_='tab-ct-flex'):\n",
    "                    title = get_text_safe(flex.find('div', class_='tab-ct-flex-title'))\n",
    "                    if title:\n",
    "                        value = ' '.join(get_text_safe(p) for p in flex.find_all('p'))\n",
    "                        program['content'][title] = value\n",
    "\n",
    "    data['programs'] = [p for p in data['programs'] if p['tab'] and (p['description'] or p['content'])]\n",
    "    data['images'] = [img['src'] for img in soup.select('div.gsc-image img') if img.get('src')]\n",
    "\n",
    "    return data\n",
    "\n",
    "# === MAIN ===\n",
    "def main():\n",
    "    os.makedirs(DETAILS_DIR, exist_ok=True)\n",
    "\n",
    "    res = requests.get(BASE_URL)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "    result = []\n",
    "    program_set = set()\n",
    "    groups = soup.find_all('div', class_='ts-ng-und')\n",
    "\n",
    "    for group in groups:\n",
    "        group_name_tag = group.find(class_='ts-ng-und-td')\n",
    "        if not group_name_tag:\n",
    "            continue\n",
    "\n",
    "        group_name = group_name_tag.get_text(strip=True)\n",
    "        majors = group.find_all('a')\n",
    "        major_list = []\n",
    "\n",
    "        for major in majors:\n",
    "            major_name = major.get_text(strip=True)\n",
    "            major_link = major['href'].replace('../../../../dai-hoc/nganh-hoc', '').strip()\n",
    "            full_url = BASE_URL + major_link\n",
    "\n",
    "            info = crawl_major_detail(full_url)\n",
    "            filename = major_link.strip('/').replace('/', '_') + '.json'\n",
    "            detail_path = os.path.join('details', filename)\n",
    "            full_detail_path = os.path.join(DETAILS_DIR, filename)\n",
    "\n",
    "            with open(full_detail_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(info, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            for p in info.get('programs', []):\n",
    "                if p.get('tab'):\n",
    "                    program_set.add(p['tab'])\n",
    "\n",
    "            major_list.append({\n",
    "                'name': major_name,\n",
    "                'link': major_link,\n",
    "                'detail': detail_path\n",
    "            })\n",
    "\n",
    "        result.append({\n",
    "            'group_name': group_name,\n",
    "            'majors': major_list\n",
    "        })\n",
    "\n",
    "    with open(os.path.join(OUTPUT_DIR, 'tdtu_majors.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    with open(os.path.join(OUTPUT_DIR, 'programme_types.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(sorted(list(program_set)), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"✅ Hoàn tất! Đã lưu vào thư mục 'output'\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4473aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã gom 43 file vào output\\major-detail.jsonl\n",
      "Đã chuyển phase1/tdtu_majors.json sang output\\tdtu_majors.json\n",
      "Đã chuyển phase1/programme_types.json sang output\\programme_types.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "DETAILS_DIR = 'phase1/details'\n",
    "OUTPUT_DIR = 'output'\n",
    "TD_MAJORS_SRC = 'phase1/tdtu_majors.json'\n",
    "PROG_TYPES_SRC = 'phase1/programme_types.json'\n",
    "TD_MAJORS_DST = os.path.join(OUTPUT_DIR, 'tdtu_majors.json')\n",
    "PROG_TYPES_DST = os.path.join(OUTPUT_DIR, 'programme_types.json')\n",
    "MAJOR_DETAIL_OUT = os.path.join(OUTPUT_DIR, 'major-detail.jsonl')\n",
    "\n",
    "def merge_major_details():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    detail_files = [f for f in os.listdir(DETAILS_DIR) if f.endswith('.json')]\n",
    "    detail_files.sort()  # Sắp xếp để dễ kiểm tra và nhất quán\n",
    "\n",
    "    with open(MAJOR_DETAIL_OUT, 'w', encoding='utf-8') as out_f:\n",
    "        for fname in detail_files:\n",
    "            full_path = os.path.join(DETAILS_DIR, fname)\n",
    "            with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                out_f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "    print(f'Đã gom {len(detail_files)} file vào {MAJOR_DETAIL_OUT}')\n",
    "\n",
    "def move_file(src, dst):\n",
    "    if os.path.exists(src):\n",
    "        shutil.move(src, dst)\n",
    "        print(f'Đã chuyển {src} sang {dst}')\n",
    "    else:\n",
    "        print(f'Không tìm thấy file {src}')\n",
    "\n",
    "def main():\n",
    "    merge_major_details()\n",
    "    move_file(TD_MAJORS_SRC, TD_MAJORS_DST)\n",
    "    move_file(PROG_TYPES_SRC, PROG_TYPES_DST)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecfdd4a",
   "metadata": {},
   "source": [
    "# GIAI ĐOẠN 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ed366",
   "metadata": {},
   "source": [
    "# PHƯƠNG THỨC TUYỂN SINH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "989c2726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang truy cập: https://admission.tdtu.edu.vn/dai-hoc/tuyen-sinh/phuong-thuc-2025\n",
      "Đã lưu ptts\\phuong-thuc-2025.json\n",
      "Đang truy cập: https://admission.tdtu.edu.vn/dai-hoc/tuyen-sinh/phuong-thuc-2024\n",
      "Đã lưu ptts\\phuong-thuc-2024.json\n",
      "Đang truy cập: https://admission.tdtu.edu.vn/dai-hoc/tuyen-sinh/phuong-thuc-2023\n",
      "Đã lưu ptts\\phuong-thuc-2023.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "\n",
    "def extract_and_save(url_objs, json_dir=\"ptts\", jsonl_dir=\"output\"):\n",
    "    os.makedirs(json_dir, exist_ok=True)\n",
    "    os.makedirs(jsonl_dir, exist_ok=True)\n",
    "    jsonl_path = os.path.join(jsonl_dir, \"phuong-thuc.jsonl\")\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as jsonl_file:\n",
    "        for obj in url_objs:\n",
    "            url = obj.get(\"url\")\n",
    "            name = obj.get(\"name\", \"output\")\n",
    "            try:\n",
    "                print(f\"Đang truy cập: {url}\")\n",
    "                resp = requests.get(url, timeout=20)\n",
    "                resp.raise_for_status()\n",
    "                soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "                content_div = soup.find(\"div\", id=\"page-main-content\")\n",
    "                if content_div:\n",
    "                    # Thay từng thẻ <a> bằng (text)[url]\n",
    "                    for a in content_div.find_all('a', href=True):\n",
    "                        text = a.get_text(strip=True)\n",
    "                        href = a['href']\n",
    "                        replacement = f\"({text})[{href}]\"\n",
    "                        a.replace_with(replacement)\n",
    "                    # Lấy text thuần, loại bỏ HTML\n",
    "                    text_content = content_div.get_text(separator=\"\\n\", strip=True)\n",
    "                    # Tạo dict để lưu\n",
    "                    data = {\n",
    "                        \"url\": url,\n",
    "                        \"name\": name,\n",
    "                        \"content\": text_content\n",
    "                    }\n",
    "                    # Lưu file .json riêng trong ptts/\n",
    "                    json_path = os.path.join(json_dir, f\"{name}.json\")\n",
    "                    with open(json_path, \"w\", encoding=\"utf-8\") as jf:\n",
    "                        json.dump(data, jf, ensure_ascii=False, indent=2)\n",
    "                    # Lưu vào .jsonl trong output/\n",
    "                    jsonl_file.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "                    print(f\"Đã lưu {json_path}\")\n",
    "                else:\n",
    "                    print(f\"Không tìm thấy div id='page-main-content' ở {url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi với {url}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = [\n",
    "        {\n",
    "            \"url\": \"https://admission.tdtu.edu.vn/dai-hoc/tuyen-sinh/phuong-thuc-2025\",\n",
    "            \"name\": \"phuong-thuc-2025\"\n",
    "        },\n",
    "        {\n",
    "            \"url\": \"https://admission.tdtu.edu.vn/dai-hoc/tuyen-sinh/phuong-thuc-2024\",\n",
    "            \"name\": \"phuong-thuc-2024\"\n",
    "        },\n",
    "        {\n",
    "            \"url\": \"https://admission.tdtu.edu.vn/dai-hoc/tuyen-sinh/phuong-thuc-2023\",\n",
    "            \"name\": \"phuong-thuc-2023\"\n",
    "        }\n",
    "    ]\n",
    "    extract_and_save(urls, json_dir=\"ptts\", jsonl_dir=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bf7a13",
   "metadata": {},
   "source": [
    "# HỌC PHÍ HỌC BỔNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe7333aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang truy cập: https://admission.tdtu.edu.vn/hoc-tai-tdtu/hoc-phi-hoc-bong-2025\n",
      "Đã crawl và ghi vào output/hoc-phi-hoc-bong.jsonl\n",
      "Đang truy cập: https://admission.tdtu.edu.vn/hoc-tai-tdtu/hoc-phi-hoc-bong-2024\n",
      "Đã crawl và ghi vào output/hoc-phi-hoc-bong.jsonl\n",
      "Đang truy cập: https://admission.tdtu.edu.vn/hoc-tai-tdtu/hoc-phi-hoc-bong-2023\n",
      "Đã crawl và ghi vào output/hoc-phi-hoc-bong.jsonl\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Nội dung học bổng thủ công cho 2023\n",
    "HOC_BONG_2023 = \"\"\"\n",
    "TRƯỜNG ĐẠI HỌC TÔN ĐỨC THẲNG\n",
    "CHÍNH SÁCH HỌC BỔNG 2023\n",
    "TDTU dành hơn 30 tỷ đồng cấp học bổng cho tân sinh viên khóa tuyền sinh 2023\n",
    "100% học phí\n",
    "năm học\n",
    "2023 - 2024\n",
    "Học bổng thủ khoa\n",
    "Học bổng dành cho học sinh\n",
    "các tỉnh hợp tác toàn diện\n",
    "Học bồng dành cho\n",
    "học sinh các trường ký kết\n",
    "Học bổng chương trình\n",
    "Đại học bằng tiếng Anh\n",
    "Học bổng ngành thu hút\n",
    "Học bổng ưu tiên xét tuyền\n",
    "Học bổng chương trình\n",
    "Liên kết đào tạo quốc tế\n",
    "Học bổng\n",
    "Phân hiệu Khánh Hòa\n",
    "dành cho tân sinh viên có điểm xét tuyển cao nhất\n",
    "dành cho học sinh thỏa điều kiện xét học bổng TDTU\n",
    "của các tỉnh hợp tác toàn diện với Trường\n",
    "(Bình Thuận, Gia Lai, Bình Định, Lâm Đồng, Quảng Ngãi)\n",
    "dành cho học sinh các trường THPT có ký kết hợp tác\n",
    "với TDTU thỏa điều kiện xét học bổng\n",
    "100% tân sinh viện chương trình dự bị tiếng Anh,\n",
    "chương trình đại học bằng tiếng Anh được cấp học bổng\n",
    "theo điều kiện của TDTU\n",
    "100% tân sinh viên các ngành thu hút\n",
    "⚫ Công tác xã hội\n",
    "Công nghệ Kỹ thuật môi trường\n",
    "Khoa học môi trường\n",
    "Bảo hộ lao động\n",
    "Quy hoạch vùng và đô thị\n",
    "Kỹ thuật xây dựng công trình giao thông\n",
    "Quản lý thể dục thể thao - Chuyên ngành Golf\n",
    "được cấp học bổng theo điều kiện của TDTU\n",
    "100% tân sinh viên nhập học vào các ngành thu hút\n",
    "theo phương thức ưu tiên xét tuyển của TDTU dành cho\n",
    "các trường ký kết, có thư giới thiệu của Hiệu trưởng\n",
    "trường THPT được xét cấp học bổng\n",
    "100% tân sinh viên chương trình dự bị tiếng Anh,\n",
    "chương trình Liên kết đào tạo quốc tế được xét cấp\n",
    "học bồng theo điều kiện của TDTU\n",
    "100% tân sinh viên học tại Phân hiệu Khánh Hòa\n",
    "được xét cấp học bổng theo điều kiện của TDTU\n",
    "Học bổng dành cho học sinh trường chuyên, trường trọng điểm\n",
    "Học sinh giỏi 03 năm các trường THPT chuyên, trường trọng điểm thỏa điều kiện xét học bổng\n",
    "Học bổng thế hệ đầu tiên học đại học\n",
    "Tân sinh viên là thế hệ đầu tiên trong gia đình học đại học trúng tuyền vào các ngành thu hút được\n",
    "xét cấp học bổng theo điều kiện của TDTU\n",
    "Các loại học bổng khác\n",
    "Học bổng cho anh/chị em ruột cùng học tại TDTU\n",
    "Học bổng cho con, anh/chị/em ruột của cán bộ công đoàn\n",
    "Học bổng cho sinh viên khuyết tật có hoàn cảnh khó khăn\n",
    "Học bổng cho tân sinh viên có tiếng Anh đầu vào cao\n",
    "và nhiều loại học bổng khác\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "\n",
    "def extract_tables_html(dd_or_div):\n",
    "    # Trả về danh sách mã html của các bảng trong thẻ dd/div\n",
    "    return [str(table) for table in dd_or_div.find_all(\"table\")]\n",
    "\n",
    "def extract_section_content_as_html(dd):\n",
    "    html_parts = []\n",
    "    for node in dd.children:\n",
    "        if getattr(node, \"name\", None) == \"table\":\n",
    "            html_parts.append(str(node))\n",
    "        elif isinstance(node, str):\n",
    "            t = node.strip()\n",
    "            if t:\n",
    "                html_parts.append(f\"<p>{t}</p>\")\n",
    "        elif hasattr(node, \"get_text\") and node.name != \"table\":\n",
    "            # Loại bỏ các bảng con để không lặp lại bảng\n",
    "            clone = node.__copy__()\n",
    "            for t in clone.find_all(\"table\"):\n",
    "                t.extract()\n",
    "            t_html = str(clone)\n",
    "            html_parts.append(t_html)\n",
    "    return \"\\n\".join(html_parts).strip()\n",
    "\n",
    "def extract_hoc_phi(dl_tag):\n",
    "    # Danh sách bảng ngoài dd (hiếm khi có)\n",
    "    tables_outside_dd = [str(table)\n",
    "                         for table in dl_tag.find_all(\"table\")\n",
    "                         if not table.find_parent(\"dd\")]\n",
    "\n",
    "    sections = []\n",
    "    dts = dl_tag.find_all(\"dt\")\n",
    "    for dt in dts:\n",
    "        section_title = dt.get_text(strip=True)\n",
    "        dd = dt.find_next_sibling(\"dd\")\n",
    "        if not dd:\n",
    "            continue\n",
    "        section_html = extract_section_content_as_html(dd)\n",
    "        sections.append({\n",
    "            \"title\": section_title,\n",
    "            \"content\": section_html\n",
    "        })\n",
    "    return sections, tables_outside_dd\n",
    "\n",
    "def extract_hoc_bong(div):\n",
    "    text = div.get_text(separator=\"\\n\", strip=True)\n",
    "    tables_html = [str(table) for table in div.find_all(\"table\")]\n",
    "    # Ghép text với các bảng html\n",
    "    content = text\n",
    "    if tables_html:\n",
    "        content += \"\\n\" + \"\\n\".join(tables_html)\n",
    "    return content, tables_html\n",
    "\n",
    "def extract_and_save(url_objs, output_path=\"output.jsonl\"):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for obj in url_objs:\n",
    "            url = obj.get(\"url\")\n",
    "            name = obj.get(\"name\", \"output\")\n",
    "            try:\n",
    "                print(f\"Đang truy cập: {url}\")\n",
    "                resp = requests.get(url, timeout=20)\n",
    "                resp.raise_for_status()\n",
    "                soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "                content_div = soup.find(\"div\", id=\"page-main-content\")\n",
    "                if not content_div:\n",
    "                    print(f\"Không tìm thấy div id='page-main-content' ở {url}\")\n",
    "                    continue\n",
    "\n",
    "                # Học phí\n",
    "                hoc_phi_dl = content_div.find(\"dl\")\n",
    "                if hoc_phi_dl:\n",
    "                    hoc_phi_sections, hoc_phi_tables = extract_hoc_phi(hoc_phi_dl)\n",
    "                else:\n",
    "                    hoc_phi_sections, hoc_phi_tables = [], []\n",
    "\n",
    "                # Học bổng\n",
    "                hoc_bong_div = content_div.find(\"div\", class_=\"cshb\")\n",
    "                if hoc_bong_div:\n",
    "                    hoc_bong_content, hoc_bong_tables = extract_hoc_bong(hoc_bong_div)\n",
    "                else:\n",
    "                    hoc_bong_content, hoc_bong_tables = \"\", []\n",
    "\n",
    "                # Ngoại lệ cho 2023\n",
    "                if \"2023\" in name and not hoc_bong_content.strip():\n",
    "                    hoc_bong_content = HOC_BONG_2023.strip()\n",
    "                    hoc_bong_tables = []\n",
    "\n",
    "                json_obj = {\n",
    "                    \"url\": url,\n",
    "                    \"name\": name,\n",
    "                    \"hoc_phi\": {\n",
    "                        \"sections\": hoc_phi_sections,     # [{ \"title\":..., \"content\": <html string>}]\n",
    "                        \"tables\": hoc_phi_tables          # [<table html>,...]\n",
    "                    },\n",
    "                    \"hoc_bong\": {\n",
    "                        \"content\": hoc_bong_content,      # text + table html nối lại\n",
    "                        \"tables\": hoc_bong_tables         # [<table html>,...]\n",
    "                    }\n",
    "                }\n",
    "                out_f.write(json.dumps(json_obj, ensure_ascii=False) + \"\\n\")\n",
    "                print(f\"Đã crawl và ghi vào {output_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi với {url}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = [\n",
    "        {\n",
    "            \"url\": \"https://admission.tdtu.edu.vn/hoc-tai-tdtu/hoc-phi-hoc-bong-2025\",\n",
    "            \"name\": \"hoc-phi-hoc-bong-2025\"\n",
    "        },\n",
    "        {\n",
    "            \"url\": \"https://admission.tdtu.edu.vn/hoc-tai-tdtu/hoc-phi-hoc-bong-2024\",\n",
    "            \"name\": \"hoc-phi-hoc-bong-2024\"\n",
    "        },\n",
    "        {\n",
    "            \"url\": \"https://admission.tdtu.edu.vn/hoc-tai-tdtu/hoc-phi-hoc-bong-2023\",\n",
    "            \"name\": \"hoc-phi-hoc-bong-2023\"\n",
    "        }\n",
    "    ]\n",
    "    extract_and_save(urls, \"output/hoc-phi-hoc-bong.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0266cc09",
   "metadata": {},
   "source": [
    "# CHUẨN HÓA DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1d9532c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files converted to ./out/\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def remove_accents(text: str) -> str:\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn'])\n",
    "    return text\n",
    "\n",
    "def slugify(text: str) -> str:\n",
    "    text = remove_accents(text)\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"\\W+\", \"_\", text)\n",
    "    text = re.sub(r\"_+\", \"_\", text)\n",
    "    text = text.strip(\"_\")\n",
    "    return text.upper()\n",
    "\n",
    "def load_jsonl(filename: str) -> List[Dict[str, Any]]:\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "def load_json(filename: str) -> Any:\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(filename: str, data: Any) -> None:\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def build_programme_title_to_id_map(programmes_json: str) -> Dict[str, str]:\n",
    "    progs = load_json(programmes_json)\n",
    "    mapping = {}\n",
    "    for p in progs:\n",
    "        if isinstance(p, dict):\n",
    "            title = p['name'].split(\":\")[0].strip().lower()\n",
    "            prog_id = p['id']\n",
    "        else:\n",
    "            title = p.split(\":\")[0].strip().lower()\n",
    "            prog_id = slugify(title)\n",
    "        mapping[title] = prog_id\n",
    "    return mapping\n",
    "\n",
    "def normalize_title(title: str) -> str:\n",
    "    text = remove_accents(title.lower().strip())\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def guess_programme_id(section_title: str, prog_map: Dict[str, str]) -> str:\n",
    "    norm_title = normalize_title(section_title)\n",
    "    for prog_title, prog_id in prog_map.items():\n",
    "        if norm_title.startswith(remove_accents(prog_title)):\n",
    "            return prog_id\n",
    "    for prog_title, prog_id in prog_map.items():\n",
    "        if remove_accents(prog_title) in norm_title:\n",
    "            return prog_id\n",
    "    return \"\"\n",
    "\n",
    "def convert_tuitions_per_programme(\n",
    "    hoc_phi_jsonl: str,\n",
    "    programmes_json: str,\n",
    "    years: List[int],\n",
    "    out_path: str\n",
    ") -> None:\n",
    "    prog_map = build_programme_title_to_id_map(programmes_json)\n",
    "    tuitions = []\n",
    "    for doc in load_jsonl(hoc_phi_jsonl):\n",
    "        year_match = re.search(r\"(\\d{4})\", doc.get(\"name\", \"\"))\n",
    "        year = year_match.group(1) if year_match else \"\"\n",
    "        hoc_phi = doc.get(\"hoc_phi\", {})\n",
    "        for section in hoc_phi.get(\"sections\", []):\n",
    "            prog_id = guess_programme_id(section.get(\"title\", \"\"), prog_map)\n",
    "            tuitions.append({\n",
    "                \"id\": f\"TUITION_{prog_id}_{year}\" if prog_id else f\"TUITION_UNKNOWN_{year}\",\n",
    "                \"year_id\": year,\n",
    "                \"programme_id\": prog_id,\n",
    "                \"name\": section.get(\"title\", \"\"),\n",
    "                \"url\": doc.get(\"url\", \"\"),\n",
    "                \"content\": section.get(\"content\", \"\")  # Lấy mã html/text đã được crawl\n",
    "            })\n",
    "        # Nếu có bảng tổng quan ngoài dd, nối tất cả bảng html lại\n",
    "        if hoc_phi.get(\"tables\"):\n",
    "            tuitions.append({\n",
    "                \"id\": f\"TUITION_OVERVIEW_{year}\",\n",
    "                \"year_id\": year,\n",
    "                \"programme_id\": \"\",\n",
    "                \"name\": f\"Tổng quan học phí {year}\",\n",
    "                \"url\": doc.get(\"url\", \"\"),\n",
    "                \"content\": \"\\n\".join(hoc_phi.get(\"tables\", []))\n",
    "            })\n",
    "    save_json(out_path, tuitions)\n",
    "\n",
    "def convert_scholarships(hoc_phi_jsonl: str, years: List[int], out_path: str) -> None:\n",
    "    scholarships = []\n",
    "    for doc in load_jsonl(hoc_phi_jsonl):\n",
    "        year_match = re.search(r\"(\\d{4})\", doc.get(\"name\", \"\"))\n",
    "        year = year_match.group(1) if year_match else \"\"\n",
    "        hoc_bong = doc.get(\"hoc_bong\", {})\n",
    "        scholarships.append({\n",
    "            \"id\": f\"SCHOLARSHIPS_{year}\",\n",
    "            \"year_id\": year,\n",
    "            \"name\": doc.get(\"name\", \"\") + \" - Học bổng\",\n",
    "            \"url\": doc.get(\"url\", \"\"),\n",
    "            \"content\": hoc_bong.get(\"content\", \"\")  # Lấy text + bảng html đã được crawl\n",
    "        })\n",
    "    save_json(out_path, scholarships)\n",
    "\n",
    "def convert_majors(major_detail_path: str, out_path: str) -> None:\n",
    "    major_list = load_jsonl(major_detail_path)\n",
    "    majors = [\n",
    "        {\n",
    "            \"id\": slugify(m[\"name\"]),\n",
    "            \"name\": m[\"name\"],\n",
    "            \"description\": m.get(\"description\", \"\")\n",
    "        }\n",
    "        for m in major_list\n",
    "    ]\n",
    "    save_json(out_path, majors)\n",
    "\n",
    "def convert_programmes(programme_types_path: str, out_path: str) -> None:\n",
    "    progs = load_json(programme_types_path)\n",
    "    result = [\n",
    "        {\n",
    "            \"id\": slugify(p.split(\":\")[0]),\n",
    "            \"name\": p\n",
    "        }\n",
    "        for p in progs\n",
    "    ]\n",
    "    save_json(out_path, result)\n",
    "\n",
    "def convert_years(years_list: List[int], out_path: str) -> None:\n",
    "    years = [{\"id\": str(y), \"name\": str(y)} for y in years_list]\n",
    "    save_json(out_path, years)\n",
    "\n",
    "def convert_major_programmes_group_years(\n",
    "    tdtu_majors_json: str,\n",
    "    major_detail_jsonl: str,\n",
    "    programme_types_json: str,\n",
    "    years: List[int],\n",
    "    out_major_path: str,\n",
    "    out_major_programmes_path: str\n",
    ") -> None:\n",
    "    majors_data = load_json(tdtu_majors_json)\n",
    "    major_detail = {slugify(m[\"name\"]): m for m in load_jsonl(major_detail_jsonl)}\n",
    "    progs = load_json(programme_types_json)\n",
    "    prog_ids = {p: slugify(p.split(\":\")[0]) for p in progs}\n",
    "    majors = []\n",
    "    major_programmes = []\n",
    "\n",
    "    for group in majors_data:\n",
    "        for m in group.get(\"majors\", []):\n",
    "            major_id = slugify(m[\"name\"])\n",
    "            detail = major_detail.get(major_id, {})\n",
    "            # ---- majors ----\n",
    "            description = detail.get(\"description\", \"\")\n",
    "            reasons = detail.get(\"reasons\", [])\n",
    "            reasons_str = \"\\n\".join(reasons) if reasons else \"\"\n",
    "            images = detail.get(\"images\", [])\n",
    "            majors.append({\n",
    "                \"id\": major_id,\n",
    "                \"name\": m[\"name\"],\n",
    "                \"description\": description,\n",
    "                \"reasons\": reasons_str,\n",
    "                \"images\": json.dumps(images, ensure_ascii=False)\n",
    "            })\n",
    "            # ---- major_programmes ----\n",
    "            # Mỗi object trong programs là một majorprogramme!\n",
    "            programs = detail.get(\"programs\", [])\n",
    "            for prog in programs:\n",
    "                tab = prog.get(\"tab\", \"\")\n",
    "                major_code = prog.get(\"major_code\", \"\")\n",
    "                programme_id = prog_ids.get(tab, \"\")\n",
    "                mp_id = f\"{major_id}_{programme_id}\"\n",
    "                node = {\n",
    "                    \"id\": mp_id,\n",
    "                    \"major_id\": major_id,\n",
    "                    \"programme_id\": programme_id,\n",
    "                    \"tab\": tab,\n",
    "                    \"major_code\": major_code,\n",
    "                    \"description\": prog.get(\"description\", \"\"),\n",
    "                    \"name\": prog.get(\"name\", m[\"name\"]),\n",
    "                    \"year_ids\": [str(y) for y in years]\n",
    "                }\n",
    "                if \"content\" in prog and isinstance(prog[\"content\"], dict):\n",
    "                    for k, v in prog[\"content\"].items():\n",
    "                        node[k] = v\n",
    "                major_programmes.append(node)\n",
    "\n",
    "    save_json(out_major_path, majors)\n",
    "    save_json(out_major_programmes_path, major_programmes)\n",
    "\n",
    "def convert_documents_phuongthuc(phuong_thuc_jsonl: str, out_path: str) -> None:\n",
    "    docs = []\n",
    "    for doc in load_jsonl(phuong_thuc_jsonl):\n",
    "        year_match = re.search(r\"(\\d{4})\", doc.get(\"name\", \"\"))\n",
    "        year = year_match.group(1) if year_match else \"\"\n",
    "        docs.append({\n",
    "            \"id\": f\"PHUONG_THUC_{year}\",\n",
    "            \"year_id\": year,\n",
    "            \"type\": \"phuong-thuc-tuyen-sinh\",\n",
    "            \"name\": doc.get(\"name\", \"\"),\n",
    "            \"url\": doc.get(\"url\", \"\"),\n",
    "            \"html\": doc.get(\"content\", \"\"),\n",
    "            \"text\": doc.get(\"content\", \"\")\n",
    "        })\n",
    "    save_json(out_path, docs)\n",
    "\n",
    "def main():\n",
    "    data_dir = \"./output\"\n",
    "    out_dir = \"./out\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    convert_majors(\n",
    "        os.path.join(data_dir, \"major-detail.jsonl\"),\n",
    "        os.path.join(out_dir, \"majors.json\")\n",
    "    )\n",
    "    convert_programmes(\n",
    "        os.path.join(data_dir, \"programme_types.json\"),\n",
    "        os.path.join(out_dir, \"programmes.json\")\n",
    "    )\n",
    "    convert_years([2023, 2024, 2025], os.path.join(out_dir, \"years.json\"))\n",
    "\n",
    "    convert_tuitions_per_programme(\n",
    "        os.path.join(data_dir, \"hoc-phi-hoc-bong.jsonl\"),\n",
    "        os.path.join(data_dir, \"programme_types.json\"),\n",
    "        [2023, 2024, 2025],\n",
    "        os.path.join(out_dir, \"tuitions.json\")\n",
    "    )\n",
    "\n",
    "    convert_scholarships(\n",
    "        os.path.join(data_dir, \"hoc-phi-hoc-bong.jsonl\"),\n",
    "        [2023, 2024, 2025],\n",
    "        os.path.join(out_dir, \"scholarships.json\")\n",
    "    )\n",
    "\n",
    "    convert_documents_phuongthuc(\n",
    "        os.path.join(data_dir, \"phuong-thuc.jsonl\"),\n",
    "        os.path.join(out_dir, \"documents.json\")\n",
    "    )\n",
    "\n",
    "    convert_major_programmes_group_years(\n",
    "        os.path.join(data_dir, \"tdtu_majors.json\"),\n",
    "        os.path.join(data_dir, \"major-detail.jsonl\"),\n",
    "        os.path.join(data_dir, \"programme_types.json\"),\n",
    "        [2023, 2024, 2025],\n",
    "        os.path.join(out_dir, \"majors.json\"),\n",
    "        os.path.join(out_dir, \"major_programmes.json\")\n",
    "    )\n",
    "\n",
    "    print(f\"All files converted to {out_dir}/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d8cb4a",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "API_BASE = \"http://localhost:5000\"\n",
    "API_KEY = \"TOKEN\"\n",
    "\n",
    "OUT_DIR = \"./out\"\n",
    "\n",
    "def load_json(fname):\n",
    "    with open(fname, encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def post_json(url, data):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    headers[\"Authorization\"] = f\"Bearer {API_KEY}\"\n",
    "    resp = requests.post(url, headers=headers, json=data)\n",
    "    try:\n",
    "        resp.raise_for_status()\n",
    "        print(f\"POST {url} OK: {resp.status_code}\")\n",
    "        print(f\"RES: {resp.json()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"POST {url} FAILED: {e}\")\n",
    "        print(resp.text)\n",
    "\n",
    "def import_majors_programmes_years():\n",
    "    majors = load_json(os.path.join(OUT_DIR, \"majors.json\"))\n",
    "    programmes = load_json(os.path.join(OUT_DIR, \"programmes.json\"))\n",
    "    years = load_json(os.path.join(OUT_DIR, \"years.json\"))\n",
    "    major_programmes = load_json(os.path.join(OUT_DIR, \"major_programmes.json\"))\n",
    "    post_json(f\"{API_BASE}/api/v2/import/majors-programmes-years\", {\n",
    "        \"majors\": majors,\n",
    "        \"programmes\": programmes,\n",
    "        \"years\": years,\n",
    "        \"major_programmes\": major_programmes\n",
    "    })\n",
    "\n",
    "def import_tuitions():\n",
    "    tuitions = load_json(os.path.join(OUT_DIR, \"tuitions.json\"))\n",
    "    # Nếu cần truyền programmes, years, có thể load và gửi cùng data\n",
    "    programmes = load_json(os.path.join(OUT_DIR, \"programmes.json\"))\n",
    "    years = load_json(os.path.join(OUT_DIR, \"years.json\"))\n",
    "    post_json(f\"{API_BASE}/api/v2/import/tuitions\", {\n",
    "        \"tuitions\": tuitions,\n",
    "        \"programmes\": programmes,\n",
    "        \"years\": years\n",
    "    })\n",
    "\n",
    "def import_scholarships():\n",
    "    scholarships = load_json(os.path.join(OUT_DIR, \"scholarships.json\"))\n",
    "    years = load_json(os.path.join(OUT_DIR, \"years.json\"))\n",
    "    post_json(f\"{API_BASE}/api/v2/import/scholarships\", {\n",
    "        \"scholarships\": scholarships,\n",
    "        \"years\": years\n",
    "    })\n",
    "\n",
    "def import_documents():\n",
    "    documents = load_json(os.path.join(OUT_DIR, \"documents.json\"))\n",
    "    years = load_json(os.path.join(OUT_DIR, \"years.json\"))\n",
    "    post_json(f\"{API_BASE}/api/v2/import/documents\", {\n",
    "        \"documents\": documents,\n",
    "        \"years\": years\n",
    "    })\n",
    "\n",
    "def main():\n",
    "    import_majors_programmes_years()\n",
    "    import_tuitions()\n",
    "    import_scholarships()\n",
    "    import_documents()\n",
    "    print(\"DONE!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
