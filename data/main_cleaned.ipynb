{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33deb537",
   "metadata": {},
   "source": [
    "# GIAI ĐOẠN 1 - CHUẨN BỊ DỮ LIỆU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56473ec",
   "metadata": {},
   "source": [
    "## IMPORT THƯ VIỆN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60cf58aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a177eef7",
   "metadata": {},
   "source": [
    "## CRAWL DỮ LIỆU NGÀNH HỌC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0627939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDTUMajorCrawler:\n",
    "    BASE_URL = 'https://admission.tdtu.edu.vn/dai-hoc/nganh-hoc'\n",
    "\n",
    "    def __init__(self, cache_dir='cache', details_subdir='details'):\n",
    "        # Tất cả output trung gian sẽ vào cache\n",
    "        self.cache_dir = cache_dir\n",
    "        self.output_dir = os.path.join(cache_dir, 'output')\n",
    "        self.phase1_dir = os.path.join(cache_dir, 'phase1')\n",
    "        self.details_dir = os.path.join(self.phase1_dir, details_subdir)\n",
    "        \n",
    "        # Các file output chính vào cache/output\n",
    "        self.majors_json_path = os.path.join(self.output_dir, 'tdtu_majors.json')\n",
    "        self.program_types_path = os.path.join(self.output_dir, 'programme_types.json')\n",
    "        self.major_detail_out = os.path.join(self.output_dir, 'major-detail.jsonl')\n",
    "\n",
    "        # Tạo tất cả thư mục cần thiết\n",
    "        os.makedirs(self.details_dir, exist_ok=True)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def get_text_safe(self, element):\n",
    "        return element.get_text(strip=True) if element else ''\n",
    "\n",
    "    def crawl_major_detail(self, url):\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        data = {}\n",
    "\n",
    "        data['name'] = self.get_text_safe(soup.find('h2', class_='title'))\n",
    "\n",
    "        description_block = soup.find('div', class_='block-nganh-1-content')\n",
    "        data['description'] = '\\n'.join(self.get_text_safe(p) for p in description_block.find_all('p')) if description_block else ''\n",
    "\n",
    "        data['reasons'] = [\n",
    "            self.get_text_safe(block.select_one('div.highlight_content > div.title'))\n",
    "            for block in soup.select('div.gsc-column')\n",
    "            if block.select_one('div.highlight_content > div.title')\n",
    "        ]\n",
    "\n",
    "        data['programs'] = []\n",
    "        tabs_container = soup.select_one('div.gsc-tabs, div.nganh-link-admission')\n",
    "        if tabs_container:\n",
    "            tab_list = tabs_container.find('ul')\n",
    "            if tab_list:\n",
    "                for li in tab_list.find_all('li'):\n",
    "                    a = li.find('a')\n",
    "                    if a:\n",
    "                        data['programs'].append({\n",
    "                            'tab': self.get_text_safe(a),\n",
    "                            'major_code': '',\n",
    "                            'description': '',\n",
    "                            'content': {}\n",
    "                        })\n",
    "\n",
    "        tab_contents = []\n",
    "        if soup.select_one('div.gsc-tabs'):\n",
    "            tab_contents = soup.select('div.gsc-tabs div.tab-content > .tab-pane')\n",
    "        elif soup.select_one('div.nganh-link-admission'):\n",
    "            nganh_container = soup.select_one('div.nganh-link-admission')\n",
    "            tab_contents = [\n",
    "                div for div in nganh_container.find_all('div', recursive=False)\n",
    "                if not div.find('ul') and div.get('id')\n",
    "            ]\n",
    "\n",
    "        for i, content in enumerate(tab_contents):\n",
    "            if i >= len(data['programs']):\n",
    "                continue\n",
    "            program = data['programs'][i]\n",
    "            tt_block = content.find('div', class_='tab-chuong-trinh-tt')\n",
    "            if tt_block:\n",
    "                strongs = [self.get_text_safe(s) for s in tt_block.find_all('strong') if self.get_text_safe(s)]\n",
    "                full_text = ' '.join(strongs).strip()\n",
    "                match = re.search(r'^(.*?)\\s*[-–—]?\\s*Mã ngành[:：]?\\s*([A-Z]{0,2}\\s*\\d{5,})', full_text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    program['name'] = match.group(1).strip()\n",
    "                    program['major_code'] = match.group(2).replace(\" \", \"\").strip()\n",
    "                else:\n",
    "                    program['name'] = full_text\n",
    "\n",
    "            program['description'] = '\\n'.join(self.get_text_safe(p) for p in content.find_all('p') if self.get_text_safe(p))\n",
    "            ct_block = content.find('div', class_='tab-chuong-trinh')\n",
    "            if ct_block:\n",
    "                for section in ct_block.find_all('div', class_='tab-ct'):\n",
    "                    for flex in section.find_all('div', class_='tab-ct-flex'):\n",
    "                        title = self.get_text_safe(flex.find('div', class_='tab-ct-flex-title'))\n",
    "                        if title:\n",
    "                            value = ' '.join(self.get_text_safe(p) for p in flex.find_all('p'))\n",
    "                            program['content'][title] = value\n",
    "\n",
    "        data['programs'] = [p for p in data['programs'] if p['tab'] and (p['description'] or p['content'])]\n",
    "        data['images'] = [img['src'] for img in soup.select('div.gsc-image img') if img.get('src')]\n",
    "        return data\n",
    "\n",
    "    def crawl_all(self):\n",
    "        result = []\n",
    "        program_set = set()\n",
    "\n",
    "        res = requests.get(self.BASE_URL)\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        groups = soup.find_all('div', class_='ts-ng-und')\n",
    "\n",
    "        for group in groups:\n",
    "            group_name_tag = group.find(class_='ts-ng-und-td')\n",
    "            if not group_name_tag:\n",
    "                continue\n",
    "\n",
    "            group_name = group_name_tag.get_text(strip=True)\n",
    "            major_list = []\n",
    "\n",
    "            for major in group.find_all('a'):\n",
    "                major_name = major.get_text(strip=True)\n",
    "                major_link = major['href'].replace('../../../../dai-hoc/nganh-hoc', '').strip()\n",
    "                full_url = self.BASE_URL + major_link\n",
    "\n",
    "                info = self.crawl_major_detail(full_url)\n",
    "                filename = major_link.strip('/').replace('/', '_') + '.json'\n",
    "                detail_path = os.path.join('details', filename)\n",
    "                full_detail_path = os.path.join(self.details_dir, filename)\n",
    "\n",
    "                with open(full_detail_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(info, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "                for p in info.get('programs', []):\n",
    "                    if p.get('tab'):\n",
    "                        program_set.add(p['tab'])\n",
    "\n",
    "                major_list.append({\n",
    "                    'name': major_name,\n",
    "                    'link': major_link,\n",
    "                    'detail': detail_path\n",
    "                })\n",
    "\n",
    "            result.append({\n",
    "                'group_name': group_name,\n",
    "                'majors': major_list\n",
    "            })\n",
    "\n",
    "        with open(self.majors_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        with open(self.program_types_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(sorted(list(program_set)), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(\"=> Crawl hoàn tất. Đã lưu majors và chương trình đào tạo vào thư mục cache/output.\")\n",
    "\n",
    "    def export_jsonl(self):\n",
    "        detail_files = [f for f in os.listdir(self.details_dir) if f.endswith('.json')]\n",
    "        detail_files.sort()\n",
    "\n",
    "        with open(self.major_detail_out, 'w', encoding='utf-8') as out_f:\n",
    "            for fname in detail_files:\n",
    "                full_path = os.path.join(self.details_dir, fname)\n",
    "                with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    out_f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "        print(f\"=> Đã export {len(detail_files)} ngành vào {self.major_detail_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f784b66",
   "metadata": {},
   "source": [
    "## CRAWL PHƯƠNG THỨC TUYỂN SINH (DOCUMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ea63b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTTSCrawler:\n",
    "    def __init__(self, urls, cache_dir=\"cache\"):\n",
    "        self.urls = urls\n",
    "        self.cache_dir = cache_dir\n",
    "        self.json_dir = os.path.join(cache_dir, \"ptts\")\n",
    "        self.output_dir = os.path.join(cache_dir, \"output\")\n",
    "        self.jsonl_path = os.path.join(self.output_dir, \"phuong-thuc.jsonl\")\n",
    "        \n",
    "        os.makedirs(self.json_dir, exist_ok=True)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def _fetch_html(self, url):\n",
    "        resp = requests.get(url, timeout=20)\n",
    "        resp.raise_for_status()\n",
    "        return resp.text\n",
    "\n",
    "    def _extract_text_content(self, html):\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        content_div = soup.find(\"div\", id=\"page-main-content\")\n",
    "        if not content_div:\n",
    "            return None\n",
    "\n",
    "        for a in content_div.find_all('a', href=True):\n",
    "            text = a.get_text(strip=True)\n",
    "            href = a['href']\n",
    "            replacement = f\"({text})[{href}]\"\n",
    "            a.replace_with(replacement)\n",
    "\n",
    "        return content_div.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    def _save_json(self, name, data):\n",
    "        json_path = os.path.join(self.json_dir, f\"{name}.json\")\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as jf:\n",
    "            json.dump(data, jf, ensure_ascii=False, indent=2)\n",
    "        print(f\"=> Đã lưu: {json_path}\")\n",
    "\n",
    "    def _append_jsonl(self, data):\n",
    "        with open(self.jsonl_path, \"a\", encoding=\"utf-8\") as jsonl_file:\n",
    "            jsonl_file.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    def extract_all(self):\n",
    "        for obj in self.urls:\n",
    "            url = obj.get(\"url\")\n",
    "            name = obj.get(\"name\", \"output\")\n",
    "            try:\n",
    "                print(f\"=> Đang truy cập: {url}\")\n",
    "                html = self._fetch_html(url)\n",
    "                text = self._extract_text_content(html)\n",
    "                if text:\n",
    "                    data = {\n",
    "                        \"url\": url,\n",
    "                        \"name\": name,\n",
    "                        \"content\": text\n",
    "                    }\n",
    "                    self._save_json(name, data)\n",
    "                    self._append_jsonl(data)\n",
    "                else:\n",
    "                    print(f\"=> Không tìm thấy nội dung từ {url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"=> Lỗi với {url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fedb1ce",
   "metadata": {},
   "source": [
    "## HỌC PHÍ VÀ HỌC BỔNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03fdab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOC_BONG_2023 = \"\"\"\n",
    "TRƯỜNG ĐẠI HỌC TÔN ĐỨC THẲNG\n",
    "CHÍNH SÁCH HỌC BỔNG 2023\n",
    "TDTU dành hơn 30 tỷ đồng cấp học bổng cho tân sinh viên khóa tuyền sinh 2023\n",
    "100% học phí\n",
    "năm học\n",
    "2023 - 2024\n",
    "Học bổng thủ khoa\n",
    "Học bổng dành cho học sinh\n",
    "các tỉnh hợp tác toàn diện\n",
    "Học bồng dành cho\n",
    "học sinh các trường ký kết\n",
    "Học bổng chương trình\n",
    "Đại học bằng tiếng Anh\n",
    "Học bổng ngành thu hút\n",
    "Học bổng ưu tiên xét tuyền\n",
    "Học bổng chương trình\n",
    "Liên kết đào tạo quốc tế\n",
    "Học bổng\n",
    "Phân hiệu Khánh Hòa\n",
    "dành cho tân sinh viên có điểm xét tuyển cao nhất\n",
    "dành cho học sinh thỏa điều kiện xét học bổng TDTU\n",
    "của các tỉnh hợp tác toàn diện với Trường\n",
    "(Bình Thuận, Gia Lai, Bình Định, Lâm Đồng, Quảng Ngãi)\n",
    "dành cho học sinh các trường THPT có ký kết hợp tác\n",
    "với TDTU thỏa điều kiện xét học bổng\n",
    "100% tân sinh viện chương trình dự bị tiếng Anh,\n",
    "chương trình đại học bằng tiếng Anh được cấp học bổng\n",
    "theo điều kiện của TDTU\n",
    "100% tân sinh viên các ngành thu hút\n",
    "⚫ Công tác xã hội\n",
    "Công nghệ Kỹ thuật môi trường\n",
    "Khoa học môi trường\n",
    "Bảo hộ lao động\n",
    "Quy hoạch vùng và đô thị\n",
    "Kỹ thuật xây dựng công trình giao thông\n",
    "Quản lý thể dục thể thao - Chuyên ngành Golf\n",
    "được cấp học bổng theo điều kiện của TDTU\n",
    "100% tân sinh viên nhập học vào các ngành thu hút\n",
    "theo phương thức ưu tiên xét tuyển của TDTU dành cho\n",
    "các trường ký kết, có thư giới thiệu của Hiệu trưởng\n",
    "trường THPT được xét cấp học bổng\n",
    "100% tân sinh viên chương trình dự bị tiếng Anh,\n",
    "chương trình Liên kết đào tạo quốc tế được xét cấp\n",
    "học bồng theo điều kiện của TDTU\n",
    "100% tân sinh viên học tại Phân hiệu Khánh Hòa\n",
    "được xét cấp học bổng theo điều kiện của TDTU\n",
    "Học bổng dành cho học sinh trường chuyên, trường trọng điểm\n",
    "Học sinh giỏi 03 năm các trường THPT chuyên, trường trọng điểm thỏa điều kiện xét học bổng\n",
    "Học bổng thế hệ đầu tiên học đại học\n",
    "Tân sinh viên là thế hệ đầu tiên trong gia đình học đại học trúng tuyền vào các ngành thu hút được\n",
    "xét cấp học bổng theo điều kiện của TDTU\n",
    "Các loại học bổng khác\n",
    "Học bổng cho anh/chị em ruột cùng học tại TDTU\n",
    "Học bổng cho con, anh/chị/em ruột của cán bộ công đoàn\n",
    "Học bổng cho sinh viên khuyết tật có hoàn cảnh khó khăn\n",
    "Học bổng cho tân sinh viên có tiếng Anh đầu vào cao\n",
    "và nhiều loại học bổng khác\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c91620f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuitionScholarshipCrawler:\n",
    "    def __init__(self, urls, cache_dir=\"cache\"):\n",
    "        self.urls = urls\n",
    "        self.cache_dir = cache_dir\n",
    "        self.output_dir = os.path.join(cache_dir, \"output\")\n",
    "        self.output_path = os.path.join(self.output_dir, \"hoc-phi-hoc-bong.jsonl\")\n",
    "        \n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def _fetch_html(self, url):\n",
    "        response = requests.get(url, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    def _extract_tables_html(self, parent):\n",
    "        return [str(table) for table in parent.find_all(\"table\")]\n",
    "\n",
    "    def _extract_section_content_as_html(self, dd):\n",
    "        html_parts = []\n",
    "        for node in dd.children:\n",
    "            if getattr(node, \"name\", None) == \"table\":\n",
    "                html_parts.append(str(node))\n",
    "            elif isinstance(node, str):\n",
    "                text = node.strip()\n",
    "                if text:\n",
    "                    html_parts.append(f\"<p>{text}</p>\")\n",
    "            elif hasattr(node, \"get_text\") and node.name != \"table\":\n",
    "                clone = node.__copy__()\n",
    "                for table in clone.find_all(\"table\"):\n",
    "                    table.extract()\n",
    "                html_parts.append(str(clone))\n",
    "        return \"\\n\".join(html_parts).strip()\n",
    "\n",
    "    def _extract_hoc_phi(self, dl_tag):\n",
    "        sections = []\n",
    "        dts = dl_tag.find_all(\"dt\")\n",
    "        for dt in dts:\n",
    "            section_title = dt.get_text(strip=True)\n",
    "            dd = dt.find_next_sibling(\"dd\")\n",
    "            if dd:\n",
    "                content = self._extract_section_content_as_html(dd)\n",
    "                sections.append({\n",
    "                    \"title\": section_title,\n",
    "                    \"content\": content\n",
    "                })\n",
    "        tables_outside = [str(table) for table in dl_tag.find_all(\"table\") if not table.find_parent(\"dd\")]\n",
    "        return sections, tables_outside\n",
    "\n",
    "    def _extract_hoc_bong(self, div):\n",
    "        text = div.get_text(separator=\"\\n\", strip=True)\n",
    "        tables_html = self._extract_tables_html(div)\n",
    "        content = text\n",
    "        if tables_html:\n",
    "            content += \"\\n\" + \"\\n\".join(tables_html)\n",
    "        return content, tables_html\n",
    "\n",
    "    def extract_all(self):\n",
    "        with open(self.output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "            for obj in self.urls:\n",
    "                url = obj.get(\"url\")\n",
    "                name = obj.get(\"name\", \"output\")\n",
    "                try:\n",
    "                    print(f\"=> Đang truy cập: {url}\")\n",
    "                    soup = self._fetch_html(url)\n",
    "                    content_div = soup.find(\"div\", id=\"page-main-content\")\n",
    "                    if not content_div:\n",
    "                        print(f\"=> Không tìm thấy nội dung ở {url}\")\n",
    "                        continue\n",
    "\n",
    "                    # Học phí\n",
    "                    hoc_phi_dl = content_div.find(\"dl\")\n",
    "                    hoc_phi_sections, hoc_phi_tables = self._extract_hoc_phi(hoc_phi_dl) if hoc_phi_dl else ([], [])\n",
    "\n",
    "                    # Học bổng\n",
    "                    hoc_bong_div = content_div.find(\"div\", class_=\"cshb\")\n",
    "                    hoc_bong_content, hoc_bong_tables = self._extract_hoc_bong(hoc_bong_div) if hoc_bong_div else (\"\", [])\n",
    "\n",
    "                    # Ngoại lệ cho 2023\n",
    "                    if \"2023\" in name and not hoc_bong_content.strip():\n",
    "                        hoc_bong_content = HOC_BONG_2023.strip()\n",
    "                        hoc_bong_tables = []\n",
    "\n",
    "                    data = {\n",
    "                        \"url\": url,\n",
    "                        \"name\": name,\n",
    "                        \"hoc_phi\": {\n",
    "                            \"sections\": hoc_phi_sections,\n",
    "                            \"tables\": hoc_phi_tables\n",
    "                        },\n",
    "                        \"hoc_bong\": {\n",
    "                            \"content\": hoc_bong_content,\n",
    "                            \"tables\": hoc_bong_tables\n",
    "                        }\n",
    "                    }\n",
    "                    out_f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "                    print(f\"=> Đã ghi: {self.output_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"=> Lỗi với {url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6fdb33",
   "metadata": {},
   "source": [
    "# GIAI ĐOẠN 2 - CHUẨN HÓA DỮ LIỆU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "084898e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDTUDataConverter:\n",
    "    def __init__(self, cache_dir: str = \"./cache\", out_dir: str = \"./out\"):\n",
    "        self.data_dir = os.path.join(cache_dir, \"output\")  # Đọc từ cache/output\n",
    "        self.out_dir = out_dir\n",
    "        os.makedirs(self.out_dir, exist_ok=True)\n",
    "\n",
    "    # ==== Tiện ích chung ====\n",
    "    @staticmethod\n",
    "    def remove_accents(text: str) -> str:\n",
    "        text = unicodedata.normalize('NFD', text)\n",
    "        return ''.join([c for c in text if unicodedata.category(c) != 'Mn'])\n",
    "\n",
    "    @classmethod\n",
    "    def slugify(cls, text: str) -> str:\n",
    "        text = cls.remove_accents(text)\n",
    "        text = text.strip().lower()\n",
    "        text = re.sub(r\"\\W+\", \"_\", text)\n",
    "        text = re.sub(r\"_+\", \"_\", text)\n",
    "        return text.strip(\"_\").upper()\n",
    "\n",
    "    @staticmethod\n",
    "    def load_jsonl(filename: str) -> List[Dict[str, Any]]:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "    @staticmethod\n",
    "    def load_json(filename: str) -> Any:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_json(filename: str, data: Any) -> None:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # ==== Chuyển đổi chính ====\n",
    "    def convert_years(self, years: List[int]):\n",
    "        data = [{\"id\": str(y), \"name\": str(y)} for y in years]\n",
    "        self.save_json(os.path.join(self.out_dir, \"years.json\"), data)\n",
    "\n",
    "    def convert_programmes(self, filename: str):\n",
    "        progs = self.load_json(filename)\n",
    "        result = [{\"id\": self.slugify(p.split(\":\")[0]), \"name\": p} for p in progs]\n",
    "        self.save_json(os.path.join(self.out_dir, \"programmes.json\"), result)\n",
    "\n",
    "    def convert_majors(self, filename: str):\n",
    "        major_list = self.load_jsonl(filename)\n",
    "        majors = [{\n",
    "            \"id\": self.slugify(m[\"name\"]),\n",
    "            \"name\": m[\"name\"],\n",
    "            \"description\": m.get(\"description\", \"\")\n",
    "        } for m in major_list]\n",
    "        self.save_json(os.path.join(self.out_dir, \"majors.json\"), majors)\n",
    "\n",
    "    def convert_tuitions(self, hoc_phi_jsonl: str, programme_json: str, years: List[int]):\n",
    "        prog_map = self._build_programme_title_to_id_map(programme_json)\n",
    "        data = []\n",
    "        for doc in self.load_jsonl(hoc_phi_jsonl):\n",
    "            year_match = re.search(r\"\\d{4}\", doc.get(\"name\", \"\"))\n",
    "            year = year_match.group(0) if year_match else \"\"\n",
    "            hoc_phi = doc.get(\"hoc_phi\", {})\n",
    "            for section in hoc_phi.get(\"sections\", []):\n",
    "                prog_id = self._guess_programme_id(section.get(\"title\", \"\"), prog_map)\n",
    "                data.append({\n",
    "                    \"id\": f\"TUITION_{prog_id}_{year}\" if prog_id else f\"TUITION_UNKNOWN_{year}\",\n",
    "                    \"year_id\": year,\n",
    "                    \"programme_id\": prog_id,\n",
    "                    \"name\": section.get(\"title\", \"\"),\n",
    "                    \"url\": doc.get(\"url\", \"\"),\n",
    "                    \"content\": section.get(\"content\", \"\")\n",
    "                })\n",
    "            if hoc_phi.get(\"tables\"):\n",
    "                data.append({\n",
    "                    \"id\": f\"TUITION_OVERVIEW_{year}\",\n",
    "                    \"year_id\": year,\n",
    "                    \"programme_id\": \"\",\n",
    "                    \"name\": f\"Tổng quan học phí {year}\",\n",
    "                    \"url\": doc.get(\"url\", \"\"),\n",
    "                    \"content\": \"\\n\".join(hoc_phi.get(\"tables\", []))\n",
    "                })\n",
    "        self.save_json(os.path.join(self.out_dir, \"tuitions.json\"), data)\n",
    "\n",
    "    def convert_scholarships(self, hoc_phi_jsonl: str):\n",
    "        data = []\n",
    "        for doc in self.load_jsonl(hoc_phi_jsonl):\n",
    "            year_match = re.search(r\"\\d{4}\", doc.get(\"name\", \"\"))\n",
    "            year = year_match.group(0) if year_match else \"\"\n",
    "            hoc_bong = doc.get(\"hoc_bong\", {})\n",
    "            data.append({\n",
    "                \"id\": f\"SCHOLARSHIPS_{year}\",\n",
    "                \"year_id\": year,\n",
    "                \"name\": doc.get(\"name\", \"\") + \" - Học bổng\",\n",
    "                \"url\": doc.get(\"url\", \"\"),\n",
    "                \"content\": hoc_bong.get(\"content\", \"\")\n",
    "            })\n",
    "        self.save_json(os.path.join(self.out_dir, \"scholarships.json\"), data)\n",
    "\n",
    "    def convert_documents_phuongthuc(self, phuong_thuc_jsonl: str):\n",
    "        data = []\n",
    "        for doc in self.load_jsonl(phuong_thuc_jsonl):\n",
    "            year_match = re.search(r\"\\d{4}\", doc.get(\"name\", \"\"))\n",
    "            year = year_match.group(0) if year_match else \"\"\n",
    "            data.append({\n",
    "                \"id\": f\"PHUONG_THUC_{year}\",\n",
    "                \"year_id\": year,\n",
    "                \"type\": \"phuong-thuc-tuyen-sinh\",\n",
    "                \"name\": doc.get(\"name\", \"\"),\n",
    "                \"url\": doc.get(\"url\", \"\"),\n",
    "                \"html\": doc.get(\"content\", \"\"),\n",
    "                \"text\": doc.get(\"content\", \"\")\n",
    "            })\n",
    "        self.save_json(os.path.join(self.out_dir, \"documents.json\"), data)\n",
    "\n",
    "    def convert_major_programmes(self, tdtu_majors_json, major_detail_jsonl, programme_types_json, years: List[int]):\n",
    "        majors_data = self.load_json(tdtu_majors_json)\n",
    "        major_detail = {self.slugify(m[\"name\"]): m for m in self.load_jsonl(major_detail_jsonl)}\n",
    "        progs = self.load_json(programme_types_json)\n",
    "        prog_ids = {p: self.slugify(p.split(\":\")[0]) for p in progs}\n",
    "        majors = []\n",
    "        major_programmes = []\n",
    "\n",
    "        for group in majors_data:\n",
    "            for m in group.get(\"majors\", []):\n",
    "                major_id = self.slugify(m[\"name\"])\n",
    "                detail = major_detail.get(major_id, {})\n",
    "                description = detail.get(\"description\", \"\")\n",
    "                reasons = detail.get(\"reasons\", [])\n",
    "                majors.append({\n",
    "                    \"id\": major_id,\n",
    "                    \"name\": m[\"name\"],\n",
    "                    \"description\": description,\n",
    "                    \"reasons\": \"\\n\".join(reasons),\n",
    "                    \"images\": json.dumps(detail.get(\"images\", []), ensure_ascii=False)\n",
    "                })\n",
    "\n",
    "                for prog in detail.get(\"programs\", []):\n",
    "                    tab = prog.get(\"tab\", \"\")\n",
    "                    major_code = prog.get(\"major_code\", \"\")\n",
    "                    programme_id = prog_ids.get(tab, \"\")\n",
    "                    mp_id = f\"{major_id}_{programme_id}\"\n",
    "                    node = {\n",
    "                        \"id\": mp_id,\n",
    "                        \"major_id\": major_id,\n",
    "                        \"programme_id\": programme_id,\n",
    "                        \"tab\": tab,\n",
    "                        \"major_code\": major_code,\n",
    "                        \"description\": prog.get(\"description\", \"\"),\n",
    "                        \"name\": prog.get(\"name\", m[\"name\"]),\n",
    "                        \"year_ids\": [str(y) for y in years]\n",
    "                    }\n",
    "                    if \"content\" in prog and isinstance(prog[\"content\"], dict):\n",
    "                        node.update(prog[\"content\"])\n",
    "                    major_programmes.append(node)\n",
    "\n",
    "        self.save_json(os.path.join(self.out_dir, \"majors.json\"), majors)\n",
    "        self.save_json(os.path.join(self.out_dir, \"major_programmes.json\"), major_programmes)\n",
    "\n",
    "    # ==== Nội bộ ====\n",
    "    def _build_programme_title_to_id_map(self, programme_json: str) -> Dict[str, str]:\n",
    "        progs = self.load_json(programme_json)\n",
    "        mapping = {}\n",
    "        for p in progs:\n",
    "            if isinstance(p, dict):\n",
    "                title = p['name'].split(\":\")[0].strip().lower()\n",
    "                prog_id = p['id']\n",
    "            else:\n",
    "                title = p.split(\":\")[0].strip().lower()\n",
    "                prog_id = self.slugify(title)\n",
    "            mapping[title] = prog_id\n",
    "        return mapping\n",
    "\n",
    "    def _guess_programme_id(self, section_title: str, prog_map: Dict[str, str]) -> str:\n",
    "        norm_title = self.remove_accents(section_title.lower().strip())\n",
    "        for prog_title, prog_id in prog_map.items():\n",
    "            if norm_title.startswith(self.remove_accents(prog_title)):\n",
    "                return prog_id\n",
    "        for prog_title, prog_id in prog_map.items():\n",
    "            if self.remove_accents(prog_title) in norm_title:\n",
    "                return prog_id\n",
    "        return \"\"\n",
    "\n",
    "    # ==== Chạy toàn bộ ====\n",
    "    def run_all(self):\n",
    "        self.convert_years([2023, 2024, 2025])\n",
    "        self.convert_programmes(os.path.join(self.data_dir, \"programme_types.json\"))\n",
    "        self.convert_majors(os.path.join(self.data_dir, \"major-detail.jsonl\"))\n",
    "        self.convert_tuitions(\n",
    "            os.path.join(self.data_dir, \"hoc-phi-hoc-bong.jsonl\"),\n",
    "            os.path.join(self.data_dir, \"programme_types.json\"),\n",
    "            [2023, 2024, 2025]\n",
    "        )\n",
    "        self.convert_scholarships(os.path.join(self.data_dir, \"hoc-phi-hoc-bong.jsonl\"))\n",
    "        self.convert_documents_phuongthuc(os.path.join(self.data_dir, \"phuong-thuc.jsonl\"))\n",
    "        self.convert_major_programmes(\n",
    "            os.path.join(self.data_dir, \"tdtu_majors.json\"),\n",
    "            os.path.join(self.data_dir, \"major-detail.jsonl\"),\n",
    "            os.path.join(self.data_dir, \"programme_types.json\"),\n",
    "            [2023, 2024, 2025]\n",
    "        )\n",
    "        print(f\"=> Dữ liệu đã được xuất ra thư mục {self.out_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bb4cba",
   "metadata": {},
   "source": [
    "# GIAI ĐOẠN 3 - IMPORT DỮ LIỆU QUA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "59c54bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDTUDataImporter:\n",
    "    def __init__(self, api_base: str, email: str, password: str, out_dir: str = \"./out\"):\n",
    "        self.api_base = api_base.rstrip(\"/\")\n",
    "        self.email = email\n",
    "        self.password = password\n",
    "        self.out_dir = out_dir\n",
    "        self.token = None\n",
    "\n",
    "    def _load_json(self, filename: str) -> Any:\n",
    "        path = os.path.join(self.out_dir, filename)\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def _post_json(self, endpoint: str, data: Dict[str, Any]) -> None:\n",
    "        if not self.token:\n",
    "            raise Exception(\"Chưa đăng nhập, vui lòng gọi login() trước khi import\")\n",
    "\n",
    "        url = f\"{self.api_base}{endpoint}\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.token}\"\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            response.raise_for_status()\n",
    "            print(f\"[SUCCESS] POST {url} ({response.status_code})\")\n",
    "            print(f\"[RESPONSE] {response.json()}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[ERROR] POST {url} failed: {e}\")\n",
    "            print(f\"[RESPONSE TEXT] {response.text}\")\n",
    "\n",
    "    def login(self) -> None:\n",
    "        \"\"\"Đăng nhập để lấy token\"\"\"\n",
    "        url = f\"{self.api_base}/api/auth/login\"\n",
    "        data = {\n",
    "            \"email\": self.email,\n",
    "            \"password\": self.password\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(url, json=data)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            if result.get(\"Code\") == 1 and \"token\" in result.get(\"Data\", {}):\n",
    "                self.token = result[\"Data\"][\"token\"]\n",
    "                print(\"[LOGIN] Đăng nhập thành công.\")\n",
    "            else:\n",
    "                raise Exception(f\"[LOGIN ERROR] Đăng nhập thất bại: {result}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise Exception(f\"[LOGIN ERROR] Không thể kết nối tới API: {e}\")\n",
    "\n",
    "    def import_majors_programmes_years(self):\n",
    "        data = {\n",
    "            \"majors\": self._load_json(\"majors.json\"),\n",
    "            \"programmes\": self._load_json(\"programmes.json\"),\n",
    "            \"years\": self._load_json(\"years.json\"),\n",
    "            \"major_programmes\": self._load_json(\"major_programmes.json\")\n",
    "        }\n",
    "        self._post_json(\"/api/v2/import/majors-programmes-years\", data)\n",
    "\n",
    "    def import_tuitions(self):\n",
    "        data = {\n",
    "            \"tuitions\": self._load_json(\"tuitions.json\"),\n",
    "            \"programmes\": self._load_json(\"programmes.json\"),\n",
    "            \"years\": self._load_json(\"years.json\")\n",
    "        }\n",
    "        self._post_json(\"/api/v2/import/tuitions\", data)\n",
    "\n",
    "    def import_scholarships(self):\n",
    "        data = {\n",
    "            \"scholarships\": self._load_json(\"scholarships.json\"),\n",
    "            \"years\": self._load_json(\"years.json\")\n",
    "        }\n",
    "        self._post_json(\"/api/v2/import/scholarships\", data)\n",
    "\n",
    "    def import_documents(self):\n",
    "        data = {\n",
    "            \"documents\": self._load_json(\"documents.json\"),\n",
    "            \"years\": self._load_json(\"years.json\")\n",
    "        }\n",
    "        self._post_json(\"/api/v2/import/documents\", data)\n",
    "\n",
    "    def import_all(self):\n",
    "        print(\"=> Đăng nhập hệ thống...\")\n",
    "        self.login()\n",
    "        print(\"=> Bắt đầu import toàn bộ dữ liệu...\")\n",
    "        self.import_majors_programmes_years()\n",
    "        self.import_tuitions()\n",
    "        self.import_scholarships()\n",
    "        self.import_documents()\n",
    "        print(\"=> Hoàn tất import dữ liệu!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a0cd3",
   "metadata": {},
   "source": [
    "# THỰC THI DỮ LIỆU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "154df32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIAI ĐOẠN 1: CRAWL DỮ LIỆU\n",
      "Crawl ngành học...\n",
      "=> Crawl hoàn tất. Đã lưu majors và chương trình đào tạo vào thư mục cache/output.\n",
      "=> Đã export 43 ngành vào cache\\output\\major-detail.jsonl\n",
      "Crawl phương thức tuyển sinh...\n",
      "=> Đang truy cập: https://admission.tdtu.edu.vn/dai-hoc/tuyen-sinh/phuong-thuc-2025\n",
      "=> Đã lưu: cache\\ptts\\phuong-thuc-2025.json\n",
      "=> Đang truy cập: https://admission.tdtu.edu.vn/dai-hoc/tuyen-sinh/phuong-thuc-2024\n",
      "=> Đã lưu: cache\\ptts\\phuong-thuc-2024.json\n",
      "=> Đang truy cập: https://admission.tdtu.edu.vn/dai-hoc/tuyen-sinh/phuong-thuc-2023\n",
      "=> Đã lưu: cache\\ptts\\phuong-thuc-2023.json\n",
      "Crawl học phí và học bổng...\n",
      "=> Đang truy cập: https://admission.tdtu.edu.vn/hoc-tai-tdtu/hoc-phi-hoc-bong-2025\n",
      "=> Đã ghi: cache\\output\\hoc-phi-hoc-bong.jsonl\n",
      "=> Đang truy cập: https://admission.tdtu.edu.vn/hoc-tai-tdtu/hoc-phi-hoc-bong-2024\n",
      "=> Đã ghi: cache\\output\\hoc-phi-hoc-bong.jsonl\n",
      "=> Đang truy cập: https://admission.tdtu.edu.vn/hoc-tai-tdtu/hoc-phi-hoc-bong-2023\n",
      "=> Đã ghi: cache\\output\\hoc-phi-hoc-bong.jsonl\n",
      "Hoàn thành crawl dữ liệu\n",
      "\n",
      "GIAI ĐOẠN 2: CHUẨN HÓA DỮ LIỆU\n",
      "=> Dữ liệu đã được xuất ra thư mục ./out/\n",
      "Hoàn thành chuẩn hóa dữ liệu\n",
      "\n",
      "GIAI ĐOẠN 3: IMPORT DỮ LIỆU\n",
      "=> Đăng nhập hệ thống...\n",
      "[LOGIN] Đăng nhập thành công.\n",
      "=> Bắt đầu import toàn bộ dữ liệu...\n",
      "[SUCCESS] POST http://192.168.1.139:5000/api/v2/import/majors-programmes-years (200)\n",
      "[RESPONSE] {'Code': 1, 'Message': 'Import majors/programmes/major_programmes/years thành công', 'Data': None}\n",
      "[SUCCESS] POST http://192.168.1.139:5000/api/v2/import/tuitions (200)\n",
      "[RESPONSE] {'Code': 1, 'Message': 'Import tuitions/programmes/years thành công', 'Data': None}\n",
      "[SUCCESS] POST http://192.168.1.139:5000/api/v2/import/scholarships (200)\n",
      "[RESPONSE] {'Code': 1, 'Message': 'Import scholarships/years thành công', 'Data': None}\n",
      "[SUCCESS] POST http://192.168.1.139:5000/api/v2/import/documents (200)\n",
      "[RESPONSE] {'Code': 1, 'Message': 'Import documents/years thành công', 'Data': None}\n",
      "=> Hoàn tất import dữ liệu!\n",
      "Hoàn thành import dữ liệu\n",
      "\n",
      "HOÀN THÀNH TOÀN BỘ QUY TRÌNH!\n"
     ]
    }
   ],
   "source": [
    "# ========== GIAI ĐOẠN 1: CRAWL DỮ LIỆU ==========\n",
    "print(\"GIAI ĐOẠN 1: CRAWL DỮ LIỆU\")\n",
    "\n",
    "# 1.1 Crawl ngành học\n",
    "print(\"Crawl ngành học...\")\n",
    "major_crawler = TDTUMajorCrawler(cache_dir='cache')\n",
    "major_crawler.crawl_all()\n",
    "major_crawler.export_jsonl()\n",
    "\n",
    "# 1.2 Crawl phương thức tuyển sinh\n",
    "print(\"Crawl phương thức tuyển sinh...\")\n",
    "ptts_urls = [\n",
    "    {\n",
    "        \"url\": \"https://admission.tdtu.edu.vn/dai-hoc/tuyen-sinh/phuong-thuc-2025\",\n",
    "        \"name\": \"phuong-thuc-2025\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://admission.tdtu.edu.vn/dai-hoc/tuyen-sinh/phuong-thuc-2024\",\n",
    "        \"name\": \"phuong-thuc-2024\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://admission.tdtu.edu.vn/dai-hoc/tuyen-sinh/phuong-thuc-2023\",\n",
    "        \"name\": \"phuong-thuc-2023\"\n",
    "    }\n",
    "]\n",
    "ptts_crawler = PTTSCrawler(urls=ptts_urls, cache_dir='cache')\n",
    "ptts_crawler.extract_all()\n",
    "\n",
    "# 1.3 Crawl học phí và học bổng\n",
    "print(\"Crawl học phí và học bổng...\")\n",
    "tuition_urls = [\n",
    "    {\n",
    "        \"url\": \"https://admission.tdtu.edu.vn/hoc-tai-tdtu/hoc-phi-hoc-bong-2025\",\n",
    "        \"name\": \"hoc-phi-hoc-bong-2025\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://admission.tdtu.edu.vn/hoc-tai-tdtu/hoc-phi-hoc-bong-2024\",\n",
    "        \"name\": \"hoc-phi-hoc-bong-2024\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://admission.tdtu.edu.vn/hoc-tai-tdtu/hoc-phi-hoc-bong-2023\",\n",
    "        \"name\": \"hoc-phi-hoc-bong-2023\"\n",
    "    }\n",
    "]\n",
    "tuition_crawler = TuitionScholarshipCrawler(urls=tuition_urls, cache_dir='cache')\n",
    "tuition_crawler.extract_all()\n",
    "\n",
    "print(\"Hoàn thành crawl dữ liệu\")\n",
    "\n",
    "# ========== GIAI ĐOẠN 2: CHUẨN HÓA DỮ LIỆU ==========\n",
    "print(\"\\nGIAI ĐOẠN 2: CHUẨN HÓA DỮ LIỆU\")\n",
    "\n",
    "converter = TDTUDataConverter(cache_dir='cache')\n",
    "converter.run_all()\n",
    "\n",
    "print(\"Hoàn thành chuẩn hóa dữ liệu\")\n",
    "\n",
    "# ========== GIAI ĐOẠN 3: IMPORT DỮ LIỆU ==========\n",
    "print(\"\\nGIAI ĐOẠN 3: IMPORT DỮ LIỆU\")\n",
    "\n",
    "# Cấu hình API - Thay đổi theo hệ thống của bạn\n",
    "importer = TDTUDataImporter(\n",
    "    api_base=\"http://192.168.1.139:5000\",\n",
    "    email=\"admin@tdtu.vn\",\n",
    "    password=\"admin\"\n",
    ")\n",
    "importer.import_all()\n",
    "\n",
    "print(\"Hoàn thành import dữ liệu\")\n",
    "print(\"\\nHOÀN THÀNH TOÀN BỘ QUY TRÌNH!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
